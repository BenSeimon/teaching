{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install panelsplit"
      ],
      "metadata": {
        "id": "F2BGW9PLAqN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from panelsplit.cross_validation import PanelSplit\n",
        "from functools import reduce"
      ],
      "metadata": {
        "id": "0Mjaa3jqEHpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (1) Train-test splits"
      ],
      "metadata": {
        "id": "pY0xn_hRC0l0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this section is to show how we can construct a valid set of (train, test) splits in the case of cross-sectional, time-series and panel data."
      ],
      "metadata": {
        "id": "NyGfL5UJ31fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a) Cross-sectional"
      ],
      "metadata": {
        "id": "z0Sn8vIAC1_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make some dummy data\n",
        "countries = pd.Index([f\"c_{x}\" for x in np.arange(1, 51)])"
      ],
      "metadata": {
        "id": "TgHj_8gMCsdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5)\n",
        "\n",
        "for i, (train_indices, test_indices) in enumerate(kf.split(countries)):\n",
        "  print(f\"Fold: {i}\")\n",
        "  print(f\"Train countries: {countries[train_indices]}\")\n",
        "  print(f\"Test countries: {countries[test_indices]}\")"
      ],
      "metadata": {
        "id": "vc7hZrPFDrMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things you should notice:\n",
        "- There is no time dimension here. Any of the above splits would be a valid way to split your data into train and test.\n",
        "\n",
        "Try:\n",
        "  - Changing `n_splits` and inspecting the lengths of `train_indices` and `test_indices`. How are n_splits and these lengths related?"
      ],
      "metadata": {
        "id": "_ZLZVzJxFuRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b) Time-series"
      ],
      "metadata": {
        "id": "7KMLkFONFm8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make some dummy data\n",
        "timesteps = pd.Index(pd.date_range(start=\"2021-01-01\", periods=36, freq=\"MS\").strftime(\"%Y%m\"))"
      ],
      "metadata": {
        "id": "FjQ131Z2FohO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits = 12 #how many splits\n",
        "test_size = 1 #how many timesteps in the test set for each split\n",
        "tss = TimeSeriesSplit(n_splits=n_splits, test_size=test_size) #sklearn TimeSeriesSplit object\n",
        "\n",
        "splits = []\n",
        "for i, (train_indices, test_indices) in enumerate(tss.split(timesteps)):\n",
        "  print(f\"Split: {i+1}\")\n",
        "  print(f\"Train timesteps: {timesteps[train_indices]}\")\n",
        "  print(f\"Timesteps: {timesteps[test_indices]}\")\n",
        "  splits.append((timesteps[train_indices], timesteps[test_indices]))\n"
      ],
      "metadata": {
        "id": "BFkoc2Lb2CDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot each split\n",
        "\n",
        "train_plot_df = pd.DataFrame(index=timesteps)\n",
        "test_plot_df = pd.DataFrame(index=timesteps)\n",
        "\n",
        "for i, (train_indices, test_indices) in enumerate(tss.split(timesteps)):\n",
        "  train_plot_df.loc[timesteps[train_indices], f'Split {i}'] = True\n",
        "  test_plot_df.loc[timesteps[test_indices], f'Split {i}'] = True\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6), dpi = 300)\n",
        "\n",
        "for i, col in enumerate(train_plot_df.columns):\n",
        "    # Find the time steps (index) where the column is True\n",
        "    true_mask = train_plot_df[col] == True\n",
        "    x_vals = train_plot_df.index[true_mask]\n",
        "\n",
        "    # Plot these time steps as scatter points on y = i\n",
        "    ax.scatter(x_vals, [i]*len(x_vals), color='blue', marker='.', s=50)\n",
        "\n",
        "for i, col in enumerate(test_plot_df.columns):\n",
        "  # Find the time steps (index) where the column is True\n",
        "  true_mask = test_plot_df[col] == True\n",
        "  x_vals = test_plot_df.index[true_mask]\n",
        "\n",
        "  # Plot these time steps as scatter points on y = i\n",
        "  ax.scatter(x_vals, [i]*len(x_vals), color='red', marker='.', s=50)\n",
        "\n",
        "\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
        "ax.set_yticks(range(len(splits)))  # Set the number of ticks on the y-axis\n",
        "ax.set_yticklabels([f'Split {i}' for i in np.arange(1, len(splits)+1)])  # Set custom labels for the y-axis\n",
        "ax.tick_params(axis='both', which='major', labelsize=14)\n",
        "ax.grid(axis='x', which='major', linestyle='--', color='lightgray', alpha=0.5)\n",
        "\n"
      ],
      "metadata": {
        "id": "WgfR-7SdGZpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things you should notice:\n",
        "- We have a time dimension now. To generate valid predictions (i.e. no data leakage), we must iteratively train on all available data and predict the next timestamp. This is called \"pseudo-out-of-sample\" forecasting.\n",
        "- Since we have 12 splits, we will also have 12 fitted estimators.\n",
        "\n",
        "Try:\n",
        "  - Changing `n_splits` and `test_size`, whilst holding `test_size`=36. Do you notice any errors? Why?\n",
        "\n",
        "Bonus:\n",
        "- There is a `gap` parameter in TimeSeriesSplit. Read the documentation (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) and then try out different configurations of `periods`, `n_splits`, `test_size`, and `gap`. If you can develop intutions here, then you are well on your way!"
      ],
      "metadata": {
        "id": "DTt8QWBN-SH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## c) Panel data"
      ],
      "metadata": {
        "id": "QzIXe_fz-Jku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a multiindex of all countries and timesteps\n",
        "panel_index = pd.MultiIndex.from_product([countries, timesteps], names=['country', 'timestep'])\n",
        "panel_index"
      ],
      "metadata": {
        "id": "YBgOojq__k9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits = 12\n",
        "test_size = 1\n",
        "panel_split = PanelSplit(\n",
        "    periods = panel_index.get_level_values('timestep'),\n",
        "    n_splits=n_splits,\n",
        "    test_size=test_size\n",
        ")"
      ],
      "metadata": {
        "id": "YjcxQ3ndSOZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the indices of the splits\n",
        "for i, (train_indices, test_indices) in enumerate(panel_split._u_periods_cv):\n",
        "  print(f\"Split: {i+1}\")\n",
        "  print(f\"Train indices: {train_indices}\")\n",
        "  print(f\"Test indices: {test_indices}\")"
      ],
      "metadata": {
        "id": "P1AFTQjRA5Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check how the splits look in the context of our multiindex\n",
        "#check the indices of the splits\n",
        "for i, (train_indices, test_indices) in enumerate(panel_split._u_periods_cv):\n",
        "  print(f\"Split: {i+1}\")\n",
        "  print(f\"Train indices: {panel_index[panel_index.get_level_values('timestep').isin(train_indices)]}\")\n",
        "  print(f\"Test indices: {panel_index[panel_index.get_level_values('timestep').isin(test_indices)]}\")"
      ],
      "metadata": {
        "id": "aM7Nt2MMe91E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things you should notice:\n",
        "- Now we have a unit (country) and time dimension! But the outcome is the same as the time-series case - we only split (train, test) pairs according to the time dimension.\n",
        "- The only difference is that each blue dot in the figure will contain 200 countries, not just 1.\n",
        "\n",
        "Try:\n",
        "  - Reading the source code at https://github.com/4Freye/panelsplit/blob/main/panelsplit/cross_validation.py. You will notice `n_splits`, `test_size`, and `gap` all appear again. Have a play around with changing these values. Also, check out the attributes of PanelSplit class to improve your understanding (top tip: `dir(panel_split)` to see all attributes and methods associated with the initialised class)."
      ],
      "metadata": {
        "id": "2gkHFEVbBezF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Target variables"
      ],
      "metadata": {
        "id": "8-OX954cGVCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this section is to understand how to generate target variables for:\n",
        "\n",
        "1.   Incidence\n",
        "2.   Onset\n",
        "\n",
        "You will also learn how your choice of horizon impacts their definition."
      ],
      "metadata": {
        "id": "Ton8zx6dGj6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NB - upload the ucdp.csv file into files (toggle on the left-hand side). You will need to do this for any given run of the notebook.\n",
        "#alternatively upload to your G Drive and mount your drive in this notebook\n",
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "aJLXZ3iiGWc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read the data\n",
        "ucdp = pd.read_csv(\"ucdp.csv\")\n",
        "ucdp = ucdp.set_index(['isocode', 'period']).sort_index() #set index to our id_vars and sort values\n",
        "\n",
        "#note: sorting values is CRITICAL when using group by operations\n",
        "ucdp"
      ],
      "metadata": {
        "id": "62ZB_VXniQTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#no need for population for demonstration. Keep it if you want a per capita target definition.\n",
        "ucdp = ucdp.drop(columns=[\"population\"])\n",
        "ucdp['fatalities_UCDP'] = ucdp['fatalities_UCDP'].astype(int)\n",
        "#rename column for convenience\n",
        "ucdp = ucdp.rename(columns={\"fatalities_UCDP\": \"violence\"})"
      ],
      "metadata": {
        "id": "gMH5dcqYiRdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TargetEngineer():\n",
        "\n",
        "  \"\"\"\n",
        "  Class to generate target variables for incidence and onset under a given horizon.\n",
        "\n",
        "  Args\n",
        "  ----\n",
        "\n",
        "  df: pd.DataFrame\n",
        "    Dataframe with at least columns that include [unit, time, y_col]\n",
        "\n",
        "  unit: str\n",
        "    Column that defines your unit. E.g. 'isocode'\n",
        "\n",
        "  time: str\n",
        "    Column that defines your time. E.g. 'period'\n",
        "\n",
        "  y_col: str\n",
        "    Column that defines your y variable. E.g. 'violence'\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, df:pd.DataFrame, unit:str, time:str, y_col:str):\n",
        "\n",
        "    self.df = df.copy()\n",
        "    self.unit = unit\n",
        "    self.time = time\n",
        "    self.y_col = y_col\n",
        "\n",
        "  def any(self, threshold:int):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to compute \"any\" variable.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "\n",
        "    threshold: int\n",
        "      Threshold to apply to self.y_col.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    pd.DataFrame\n",
        "      Dataframe with any variable.\n",
        "\n",
        "    str\n",
        "      Name of the any variable.\n",
        "    \"\"\"\n",
        "\n",
        "    any_col = f\"any{self.y_col}_th{threshold}\"\n",
        "    self.df[any_col] = (self.df[self.y_col] > threshold).astype(int)\n",
        "    return self.df.copy(), any_col\n",
        "\n",
        "  def incidence(self, threshold:int, horizon:int):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to compute incidence target variable based on the specific threshold and horizon.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "\n",
        "    threshold: int\n",
        "      Threshold to apply to self.y_col.\n",
        "\n",
        "    horizon: int\n",
        "      Forecasting horizon (assumes aggregated window).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    pd.DataFrame\n",
        "      Dataframe with y_col, any_col, and target_col.\n",
        "    \"\"\"\n",
        "\n",
        "    #make the any variable\n",
        "    df, any_col = self.any(threshold)\n",
        "\n",
        "    #get the rolling max value of your any variable over the specified horizon\n",
        "    any_col_max = f\"{any_col}_max\"\n",
        "    df[any_col_max] = self.df.groupby(self.unit)[any_col]. \\\n",
        "            transform(lambda x: x.rolling(window=horizon, min_periods = horizon).max())\n",
        "\n",
        "    #shift any_col_max by the specified horizon to get your incidence target variable\n",
        "    target_col = f\"inc_{any_col}_h{horizon}\"\n",
        "    df[target_col] = df.groupby(self.unit)[any_col_max].transform(lambda x: x.shift(-horizon))\n",
        "\n",
        "    return df[[self.y_col, any_col, target_col]]\n",
        "\n",
        "  def onset(self, threshold:int, horizon:int):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to compute onset target variable based on the specific threshold and horizon.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "\n",
        "    threshold: int\n",
        "      Threshold to apply to self.y_col.\n",
        "\n",
        "    horizon: int\n",
        "      Forecasting horizon (assumes aggregated window).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    pd.DataFrame\n",
        "      Dataframe with y_col, any_col, and target_col.\n",
        "    \"\"\"\n",
        "\n",
        "    #make the any variable\n",
        "    df, any_col = self.any(threshold)\n",
        "\n",
        "    def _onset(x:pd.Series, h:int):\n",
        "\n",
        "      \"\"\"\n",
        "      Function to compute onset target variable for a single unit\n",
        "\n",
        "      Args\n",
        "      ---\n",
        "\n",
        "      x: pd.Series\n",
        "        The \"any\" variable for a single unit.\n",
        "\n",
        "      h: int\n",
        "        Forecasting horizon (assumes aggregated window).\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "\n",
        "      pd.Series\n",
        "        The onset target variable for a single unit.\n",
        "      \"\"\"\n",
        "\n",
        "      index = x.index #get the index\n",
        "      x = list(x) #convert to list\n",
        "      y = [] #empty list for storing the onset target\n",
        "      for i in range(len(x)): #iterate over every element in x\n",
        "          i0 = i+1 #index of the next period\n",
        "          i1 = i0+h #index at the end of the forecast horizon\n",
        "          if i1 <= len(x) and x[i]==0: #first if condition is to handle the last h observations. Second condition states if any==0.\n",
        "              y.append(np.max(x[i0:i1])) #append the max of the any column in the next h periods, assuming any==0 currently\n",
        "          else:\n",
        "              y.append(np.nan) #otherwise append NA\n",
        "      return pd.Series(y, index)\n",
        "\n",
        "    target_col = f\"ons_{any_col}_h{horizon}\"\n",
        "    df[target_col] = self.df.groupby(self.unit)[any_col].transform(lambda x: _onset(x, horizon))\n",
        "\n",
        "    return df[[self.y_col, any_col, target_col]]"
      ],
      "metadata": {
        "id": "0GJFrep-rfQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize our class\n",
        "target_engineer = TargetEngineer(df=ucdp, unit='isocode', time='period', y_col=\"violence\")"
      ],
      "metadata": {
        "id": "X6wWxI4LtqOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a) Step-by-step"
      ],
      "metadata": {
        "id": "yzJJOLPbj3b-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: define any_variable\n",
        "\n",
        "threshold = 0 #threshold=0 means any violence\n",
        "any_df, any_col = target_engineer.any(threshold)\n",
        "\n",
        "print(any_col)\n",
        "any_df"
      ],
      "metadata": {
        "id": "XVb__gTaiXnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: define horizon\n",
        "horizon = 3"
      ],
      "metadata": {
        "id": "39PVkORqjZcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3a: get incidence target\n",
        "inc_df = target_engineer.incidence(threshold=threshold, horizon=horizon)\n",
        "cond1 = (inc_df.index.get_level_values('isocode') == 'LBY')\n",
        "inc_df.loc[cond1].tail(18)"
      ],
      "metadata": {
        "id": "pRsciGc8rWcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3b: get onset target\n",
        "ons_df = target_engineer.onset(threshold=threshold, horizon=horizon)\n",
        "ons_df\n"
      ],
      "metadata": {
        "id": "Eeepb1WJu2gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merge dataframes\n",
        "dfs_list = [ucdp, any_df.drop(columns = ['violence']), inc_df.drop(columns = ['violence', any_col]), ons_df.drop(columns = ['violence', any_col])]\n",
        "target_df = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='inner'), dfs_list)\n",
        "target_df"
      ],
      "metadata": {
        "id": "L6H7z1-zIASO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Libya example\n",
        "cond1 = (target_df.index.get_level_values('isocode') == 'LBY')\n",
        "cond2 = (target_df.index.get_level_values('period') >= 202101)\n",
        "cond3 = (target_df.index.get_level_values('period') <= 202301)\n",
        "target_df.loc[cond1&cond2&cond3].tail(50)"
      ],
      "metadata": {
        "id": "H1JKeuVzIYRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things you should notice:\n",
        "\n",
        "- Take a look at the Libya example for a `threshold=0` and `horizon=3` using this one-liner `df.loc[df.index.get_level_values('isocode') == 'LBY'].tail(50)`. Scroll through by eye and make you understand when and why the target variable is 0 and when it is 1.\n",
        "- Where do you have NAs in the incidence target? Why?\n",
        "- Where do you have NAs in the onset target? Why?\n",
        "\n",
        "Try:\n",
        "  - Investigating class imbalance for incidence and onset. Make sure you understand why they are different.\n",
        "  - Modifying the threshold and horizon. And compute class imabalance again. What is the relationship between i) class imbalance and a higher threshold; and ii) class imbalance and horizon. Do this for incidence first, and then onset.\n",
        "\n",
        "Bonus:\n",
        "  - What about a per capita definition of violence? Why do you think this is better/worse? What data-driven approach would you use to set a reasonable threshold?"
      ],
      "metadata": {
        "id": "vbnvtAVv306J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Train-test splits and target variable"
      ],
      "metadata": {
        "id": "AbQxeUOILQeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this section is to understand how your choice of horizon dictates how you should construct your (train, test) splits."
      ],
      "metadata": {
        "id": "_9P6iwzdL2VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first we will need a target dataframe. Let's do this again for incidence to be sure.\n",
        "target_engineer = TargetEngineer(df=ucdp, unit='isocode', time='period', y_col=\"violence\")\n",
        "threshold = 0 #define threshold\n",
        "horizon = 3 #define horizon\n",
        "inc_target_df = target_engineer.incidence(threshold=threshold, horizon=horizon)[f\"inc_anyviolence_th{threshold}_h{horizon}\"]\n",
        "inc_target_df\n"
      ],
      "metadata": {
        "id": "n8age0uRL-6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and remember that we have panel split\n",
        "n_splits = 12\n",
        "test_size = 1\n",
        "ps = PanelSplit(\n",
        "    periods = inc_target_df.index.get_level_values('period'),\n",
        "    n_splits=n_splits,\n",
        "    test_size=test_size\n",
        ")"
      ],
      "metadata": {
        "id": "UGUIy91MMN_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ps._u_periods_cv` is a list of tuples. Each tuple represents (train_indices, test_indices) for a given split.\n"
      ],
      "metadata": {
        "id": "LMcpfgZaMy6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(ps._u_periods_cv[0]) #the last split\n",
        "display(inc_target_df.loc[inc_target_df.index.get_level_values('period').isin(ps._u_periods_cv[-1][0])]) #subset the target dataframe to the train indices of the last split\n",
        "display(inc_target_df.loc[inc_target_df.index.get_level_values('period').isin(ps._u_periods_cv[-1][1])]) #subset the target dataframe to the test indices of the last split"
      ],
      "metadata": {
        "id": "b9u4Fmj52ugU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(ps._u_periods_cv[0]) #the first split\n",
        "display(inc_target_df.loc[inc_target_df.index.get_level_values('period').isin(ps._u_periods_cv[0][0])]) #subset the target dataframe to the train indices of the first split\n",
        "display(inc_target_df.loc[inc_target_df.index.get_level_values('period').isin(ps._u_periods_cv[0][1])]) #subset the target dataframe to the test indices of the first split"
      ],
      "metadata": {
        "id": "dofljzfKMvYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things you should notice:\n",
        "\n",
        "- We have data leakage here!!!!!\n",
        "- Cast your mind back and imagine you are forecating with all available information up until the end of 202303.\n",
        "- We are forecasting an aggregate outcome for the next 3 months. In other words, will there be any violence in any of 202304, 202305 or 202306.\n",
        "- But, when training we need to simulate the known information set at that time.\n",
        "- Therefore, the target should be unknown in our training data for each of 202301 and 202302.\n",
        "\n",
        "It would be tiresome to recursively compute the correct target for every timestep. Instead, we can use the `gap` parameter in panelsplit to handle this.\n"
      ],
      "metadata": {
        "id": "O7gUlZwONGmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#and remember that we have panel split\n",
        "n_splits = 12\n",
        "test_size = 1\n",
        "ps = PanelSplit(\n",
        "    periods = inc_target_df.index.get_level_values('period'),\n",
        "    n_splits=n_splits,\n",
        "    test_size=test_size,\n",
        "    gap = 2\n",
        ")"
      ],
      "metadata": {
        "id": "xrnCR5vpN9qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(ps._u_periods_cv[-1]) #the last split\n",
        "display(inc_target_df.loc[inc_target_df.index.get_level_values('period').isin(ps._u_periods_cv[-1][0])]) #subset the target dataframe to the train indices of the last split\n",
        "display(inc_target_df.loc[inc_target_df.index.get_level_values('period').isin(ps._u_periods_cv[-1][1])]) #subset the target dataframe to the test indices of the last split"
      ],
      "metadata": {
        "id": "c5N910UY2VXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(ps._u_periods_cv[0]) #the first split\n",
        "display(inc_target_df.loc[inc_target_df.index.get_level_values('period').isin(ps._u_periods_cv[0][0])]) #subset the target dataframe to the train indices of the first split\n",
        "display(inc_target_df.loc[inc_target_df.index.get_level_values('period').isin(ps._u_periods_cv[0][1])]) #subset the target dataframe to the test indices of the first split"
      ],
      "metadata": {
        "id": "jUvU33zCPmkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things you should notice:\n",
        "\n",
        "- 202301 and 202302 have now dropped out of our training set - hooray!\n",
        "\n",
        "Try:\n",
        "- Inspecting what this looks like for other splits. Does everything look correct?\n",
        "- Look at the last split. Assuming you have correctly generated your target dataframe, I find this extremely useful for intuitution. The last timestep in the training dataframe should be exactly the last point at which you can compute your target.\n",
        "- Modifying your horizon - then investigate how to set n_splits and the gap parameter correctly."
      ],
      "metadata": {
        "id": "G0GuUlW_Pu0_"
      }
    }
  ]
}