{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBpQhnQ6bIrX"
      },
      "outputs": [],
      "source": [
        "!pip install panelsplit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKFRCf9Bbl9f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from panelsplit.cross_validation import PanelSplit\n",
        "from panelsplit.application import cross_val_fit_predict\n",
        "from typing import Union\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from functools import reduce\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrfj6ECecaev"
      },
      "outputs": [],
      "source": [
        "class TargetEngineer():\n",
        "\n",
        "  \"\"\"\n",
        "  Class to generate target variables for incidence and onset under a given horizon.\n",
        "\n",
        "  Args\n",
        "  ----\n",
        "\n",
        "  df: pd.DataFrame\n",
        "    Dataframe with at least columns that include [unit, time, y_col]\n",
        "\n",
        "  unit: str\n",
        "    Column that defines your unit. E.g. 'isocode'\n",
        "\n",
        "  time: str\n",
        "    Column that defines your time. E.g. 'period'\n",
        "\n",
        "  y_col: str\n",
        "    Column that defines your y variable. E.g. 'violence'\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, df:pd.DataFrame, unit:str, time:str, y_col:str):\n",
        "\n",
        "    self.df = df.copy()\n",
        "    self.unit = unit\n",
        "    self.time = time\n",
        "    self.y_col = y_col\n",
        "\n",
        "  def any(self, threshold:int):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to compute \"any\" variable.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "\n",
        "    threshold: int\n",
        "      Threshold to apply to self.y_col.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    pd.DataFrame\n",
        "      Dataframe with any variable.\n",
        "\n",
        "    str\n",
        "      Name of the any variable.\n",
        "    \"\"\"\n",
        "\n",
        "    any_col = f\"any{self.y_col}_th{threshold}\"\n",
        "    self.df[any_col] = (self.df[self.y_col] > threshold).astype(int)\n",
        "    return self.df.copy(), any_col\n",
        "\n",
        "  def incidence(self, threshold:int, horizon:int):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to compute incidence target variable based on the specific threshold and horizon.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "\n",
        "    threshold: int\n",
        "      Threshold to apply to self.y_col.\n",
        "\n",
        "    horizon: int\n",
        "      Forecasting horizon (assumes aggregated window).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    pd.DataFrame\n",
        "      Dataframe with y_col, any_col, and target_col.\n",
        "    \"\"\"\n",
        "\n",
        "    #make the any variable\n",
        "    df, any_col = self.any(threshold)\n",
        "\n",
        "    #get the rolling max value of your any variable over the specified horizon\n",
        "    any_col_max = f\"{any_col}_max\"\n",
        "    df[any_col_max] = self.df.groupby(self.unit)[any_col]. \\\n",
        "            transform(lambda x: x.rolling(window=horizon, min_periods = horizon).max())\n",
        "\n",
        "    #shift any_col_max by the specified horizon to get your incidence target variable\n",
        "    target_col = f\"inc_{any_col}_h{horizon}\"\n",
        "    df[target_col] = df.groupby(self.unit)[any_col_max].transform(lambda x: x.shift(-horizon))\n",
        "\n",
        "    return df[[self.y_col, any_col, target_col]]\n",
        "\n",
        "  def onset(self, threshold:int, horizon:int):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to compute onset target variable based on the specific threshold and horizon.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "\n",
        "    threshold: int\n",
        "      Threshold to apply to self.y_col.\n",
        "\n",
        "    horizon: int\n",
        "      Forecasting horizon (assumes aggregated window).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    pd.DataFrame\n",
        "      Dataframe with y_col, any_col, and target_col.\n",
        "    \"\"\"\n",
        "\n",
        "    #make the any variable\n",
        "    df, any_col = self.any(threshold)\n",
        "\n",
        "    def _onset(x:pd.Series, h:int):\n",
        "\n",
        "      \"\"\"\n",
        "      Function to compute onset target variable for a single unit\n",
        "\n",
        "      Args\n",
        "      ---\n",
        "\n",
        "      x: pd.Series\n",
        "        The \"any\" variable for a single unit.\n",
        "\n",
        "      h: int\n",
        "        Forecasting horizon (assumes aggregated window).\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "\n",
        "      pd.Series\n",
        "        The onset target variable for a single unit.\n",
        "      \"\"\"\n",
        "\n",
        "      index = x.index #get the index\n",
        "      x = list(x) #convert to list\n",
        "      y = [] #empty list for storing the onset target\n",
        "      for i in range(len(x)): #iterate over every element in x\n",
        "          i0 = i+1 #index of the next period\n",
        "          i1 = i0+h #index at the end of the forecast horizon\n",
        "          if i1 <= len(x) and x[i]==0: #first if condition is to handle the last h observations. Second condition states if any==0.\n",
        "              y.append(np.max(x[i0:i1])) #append the max of the any column in the next h periods, assuming any==0 currently\n",
        "          else:\n",
        "              y.append(np.nan) #otherwise append NA\n",
        "      return pd.Series(y, index)\n",
        "\n",
        "    target_col = f\"ons_{any_col}_h{horizon}\"\n",
        "    df[target_col] = self.df.groupby(self.unit)[any_col].transform(lambda x: _onset(x, horizon))\n",
        "\n",
        "    return df[[self.y_col, any_col, target_col]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIGDMEztcbv7"
      },
      "outputs": [],
      "source": [
        "class FeatureEngineer:\n",
        "\n",
        "    \"\"\"\n",
        "    This is a class that contains general methods that can be applied to a DataFrame to create new features. Examples of such methods include creating lagged variables, rolling min/mean/max/sum and weighted rolling mean/sum.\n",
        "    The methods in this class are designed to be used in a pipeline to create new features for a given DataFrame.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    groupby_cols : Union[str, list]\n",
        "        A str or list of columns to group by\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "\n",
        "    lag(input_df:pd.DataFrame, y_col:str, lags:list):\n",
        "        This is a method that creates lagged variables for a given column in a DataFrame.\n",
        "\n",
        "    rolling_sum(input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "        This is a method that creates the rolling sum of specified windows for a given column in a DataFrame.\n",
        "\n",
        "    rolling_mean(input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "        This is a method that creates the rolling mean of specified windows for a given column in a DataFrame.\n",
        "\n",
        "    rolling_min(input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "        This is a method that creates the rolling min of specified windows for a given column in a DataFrame.\n",
        "\n",
        "    rolling_max(input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "        This is a method that creates the rolling max of specified windows for a given column in a DataFrame.\n",
        "\n",
        "    rolling_std(input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "        This is a method that creates the rolling standard deviation of specified windows for a given column in a DataFrame.\n",
        "\n",
        "    create_exponential_weights(window_size, alpha=0.8):\n",
        "        This is a method that enables generating \"rolling\" exponential weights for a given window size.\n",
        "\n",
        "    weighted_rolling_sum(input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False, alpha = 0.8):\n",
        "        This is a method that creates the weighted rolling sum of specified windows for a given column in a DataFrame.\n",
        "\n",
        "    weighted_rolling_mean(input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False, alpha = 0.8):\n",
        "        This is a method that creates the weighted rolling mean of specified windows for a given column in a DataFrame.\n",
        "\n",
        "    count_since(input_df:pd.DataFrame, y_col:str, thresholds:list, shift_knowledge:int = None):\n",
        "        This is a method that counts the number of periods since a variable has been above a given threshold.\n",
        "\n",
        "    ongoing(input_df:pd.DataFrame, y_col:str, thresholds:list, shift_knowledge:int = None):\n",
        "        This is a method that represents a sequential count of the number of periods for which a variable has been above a given threshold.\n",
        "\n",
        "    Notes:\n",
        "    -------\n",
        "    Be very careful with NAs when using the count_since_thresh and ongoing_episode methods.\n",
        "    The way we are computing things here (i.e. using a > th condition) means they are treated as a 0/False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, groupby_cols: Union[str, list]):\n",
        "\n",
        "        self.groupby_cols = groupby_cols\n",
        "\n",
        "    def _index_check(self, df:pd.DataFrame):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that checks if the index of a DataFrame is sorted correctly.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param df: The DataFrame to check.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The DataFrame with a sorted index.\n",
        "        \"\"\"\n",
        "\n",
        "        assert df.index.is_monotonic_increasing, \"The index of the DataFrame should be monotonically increasing.\"\n",
        "\n",
        "    def lag(self, input_df:pd.DataFrame, y_col:str, lags:list):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that creates lagged variables for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create lagged variables.\n",
        "        :param lags: A list of lag values to create.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the lagged variables appended.\n",
        "        \"\"\"\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_basic_lag' + str(lag) for lag in lags]\n",
        "        for idx, lag in enumerate(lags):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].shift(lag)\n",
        "        return df\n",
        "\n",
        "    def rolling_sum(self, input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that creates the rolling sum of specified windows for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create lagged variables.\n",
        "        :param groupby_cols: A list of columns to group by.\n",
        "        :param windows: A list of windows to generate a rolling sum for.\n",
        "        :param closed: A string indicating the side of the window interval to close on. Closed = 'left' omits the current observation.\n",
        "        :param return_logs: A boolean indicating whether to return the log of the rolling sum.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the rolling sum variables appended.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_rolling_sum' + str(w) for w in windows]\n",
        "\n",
        "        for idx, w in enumerate(windows):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].rolling(w, min_periods=1, closed = closed).sum().values\n",
        "            if return_logs:\n",
        "                df['ln_' + col_names[idx]] = np.log1p(df[col_names[idx]])\n",
        "                df = df.drop(col_names[idx], axis = 1)\n",
        "        return df\n",
        "\n",
        "    def rolling_mean(self, input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that creates the rolling mean of specified windows for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create rolling variables.\n",
        "        :param windows: A list of windows to generate a rolling mean for.\n",
        "        :param closed: A string indicating the side of the window interval to close on. Closed = 'left' omits the current observation.\n",
        "        :param return_logs: A boolean indicating whether to return the log of the rolling mean.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the rolling mean variables appended.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_rolling_mean' + str(w) for w in windows]\n",
        "\n",
        "        for idx, w in enumerate(windows):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].rolling(w, min_periods=1, closed = closed).mean().values\n",
        "            if return_logs:\n",
        "                df['ln_' + col_names[idx]] = np.log1p(df[col_names[idx]])\n",
        "                df = df.drop(col_names[idx], axis = 1)\n",
        "        return df\n",
        "\n",
        "    def rolling_min(self, input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that creates the rolling min of specified windows for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create rolling variables.\n",
        "        :param windows: A list of windows to generate a rolling min for.\n",
        "        :param closed: A string indicating the side of the window interval to close on. Closed = 'left' omits the current observation.\n",
        "        :param return_logs: A boolean indicating whether to return the log of the rolling min.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the rolling min variables appended.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_rolling_min' + str(w) for w in windows]\n",
        "\n",
        "        for idx, w in enumerate(windows):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].rolling(w, min_periods=1, closed = closed).min().values\n",
        "            if return_logs:\n",
        "                df['ln_' + col_names[idx]] = np.log1p(df[col_names[idx]])\n",
        "                df = df.drop(col_names[idx], axis = 1)\n",
        "        return df\n",
        "\n",
        "    def rolling_max(self, input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that creates the rolling max of specified windows for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create rolling variables.\n",
        "        :param windows: A list of windows to generate a rolling max for.\n",
        "        :param closed: A string indicating the side of the window interval to close on. Closed = 'left' omits the current observation.\n",
        "        :param return_logs: A boolean indicating whether to return the log of the rolling max.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the rolling max variables appended.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_rolling_max' + str(w) for w in windows]\n",
        "\n",
        "        for idx, w in enumerate(windows):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].rolling(w, min_periods=1, closed = closed).max().values\n",
        "            if return_logs:\n",
        "                df['ln_' + col_names[idx]] = np.log1p(df[col_names[idx]])\n",
        "                df = df.drop(col_names[idx], axis = 1)\n",
        "        return df\n",
        "\n",
        "    def rolling_std(self, input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that creates the rolling standard deviation of specified windows for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create rolling variables.\n",
        "        :param windows: A list of windows to generate a rolling standard deviation for.\n",
        "        :param closed: A string indicating the side of the window interval to close on. Closed = 'left' omits the current observation.\n",
        "        :param return_logs: A boolean indicating whether to return the log of the rolling standard deviation.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the rolling standard deviation variables appended.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_rolling_std' + str(w) for w in windows]\n",
        "\n",
        "        for idx, w in enumerate(windows):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].rolling(w, min_periods=1, closed = closed).std().values\n",
        "            if return_logs:\n",
        "                df['ln_' + col_names[idx]] = np.log1p(df[col_names[idx]])\n",
        "                df = df.drop(col_names[idx], axis = 1)\n",
        "        return df\n",
        "\n",
        "    def _create_exponential_weights(self, window_size, alpha=0.8):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that enables generating \"rolling\" exponential weights for a given window size.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param window_size: The size of the window for which weights are calculated.\n",
        "        :param alpha: The decay factor for weights, defaults to 0.5.\n",
        "                    A higher alpha discounts older observations faster.\n",
        "\n",
        "        Returns:\n",
        "        -----\n",
        "        :return: A numpy array of weights.\n",
        "        \"\"\"\n",
        "\n",
        "        weights = alpha ** np.arange(window_size)\n",
        "        normalized_weights = weights / weights.sum()\n",
        "        return normalized_weights[::-1]\n",
        "\n",
        "    def weighted_rolling_sum(self, input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False, alpha = 0.8):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that creates the weighted rolling sum of specified windows for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create weighted rolling variables.\n",
        "        :param groupby_cols: A list of columns to group by.\n",
        "        :param windows: A list of windows to generate a weighted rolling sum for.\n",
        "        :param closed: A string indicating the side of the window interval to close on. Closed = 'left' omits the current observation.\n",
        "        :param return_logs:  A boolean indicating whether to return the log of the weighted rolling sum.\n",
        "        :param alpha: The decay factor for weights, defaults to 0.8. A higher alpha discounts older observations faster.\n",
        "\n",
        "        Returns:\n",
        "        -----\n",
        "        :return: The original DataFrame with the weighted rolling sum variables appended.\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_weighted_rolling_sum' + str(w) for w in windows]\n",
        "\n",
        "        for idx, w in enumerate(windows):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].rolling(w, min_periods=1, closed = closed).apply(lambda x: np.sum(self._create_exponential_weights(len(x), alpha) * x), raw = True).values\n",
        "            if return_logs:\n",
        "                df['ln_' + col_names[idx]] = np.log1p(df[col_names[idx]])\n",
        "                df = df.drop(col_names[idx], axis = 1)\n",
        "        return df\n",
        "\n",
        "    def weighted_rolling_mean(self, input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False, alpha = 0.8):\n",
        "        \"\"\"\n",
        "        This is a method that creates the weighted rolling mean of specified windows for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create weighted rolling variables.\n",
        "        :param windows: A list of windows to generate a weighted rolling mean for.\n",
        "        :param closed: A string indicating the side of the window interval to close on. Closed = 'left' omits the current observation.\n",
        "        :param return_logs:  A boolean indicating whether to return the log of the weighted rolling mean.\n",
        "        :param alpha: The decay factor for weights, defaults to 0.8. A higher alpha discounts older observations faster.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the weighted rolling mean variables appended.\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_weighted_rolling_mean' + str(w) for w in windows]\n",
        "\n",
        "        for idx, w in enumerate(windows):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].rolling(w, min_periods=1, closed = closed).apply(lambda x: np.sum(self._create_exponential_weights(len(x), alpha) * x) / len(x), raw = True).values\n",
        "            if return_logs:\n",
        "                df['ln_' + col_names[idx]] = np.log1p(df[col_names[idx]])\n",
        "                df = df.drop(col_names[idx], axis = 1)\n",
        "        return df\n",
        "\n",
        "    def _count_since(self, x: pd.Series):\n",
        "        \"\"\"\n",
        "        This is a method that counts the number of periods since a variable has been 1.\n",
        "\n",
        "        :param x: A pandas Series containing the target variable.\n",
        "\n",
        "        Returns:\n",
        "        - y (list): A list containing the number of periods since the target variable has been 1.\n",
        "        \"\"\"\n",
        "\n",
        "        x = list(x)\n",
        "        y = []\n",
        "        for n in range(0, len(x)):\n",
        "            if (x[n] == 0) & (n == 0):\n",
        "                y.append(1) # if it starts with no flows\n",
        "            elif x[n] == 1:\n",
        "                y.append(0) # reset to 0 if flows\n",
        "            else:\n",
        "                y.append(y[n-1]+1) # add 1 if no flows\n",
        "        return y\n",
        "\n",
        "    def since(self, input_df:pd.DataFrame, y_col:str, thresholds:list, shift_knowledge:int = None):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that counts the number of periods since a variable has been above a given threshold.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create the count since variable.\n",
        "        :param thresholds: A list of thresholds to count since.\n",
        "        :param shift_knowledge: An integer defining by how many periods to shift the count since variable.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the count since variables appended.\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "\n",
        "        binary_col_names = [y_col + '_above' + str(th) for th in thresholds]\n",
        "        col_names = [y_col + '_since_' + str(th) for th in thresholds]\n",
        "\n",
        "        for idx, th in enumerate(thresholds):\n",
        "            df[binary_col_names[idx]] = (df[y_col] > th).astype(int)\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[binary_col_names[idx]].transform(self._count_since)\n",
        "\n",
        "            if shift_knowledge is None:\n",
        "                pass\n",
        "            else:\n",
        "                #in case we need to shift by one since we don't know the y_col in current period\n",
        "                df[binary_col_names[idx]] = df.groupby(self.groupby_cols)[[binary_col_names[idx]]].shift(shift_knowledge)\n",
        "                df[col_names[idx]] = df.groupby(self.groupby_cols)[col_names[idx]].shift(shift_knowledge)\n",
        "        return df[[y_col, *[x for x in df.columns if 'since' in x]]]\n",
        "\n",
        "    def _count_ongoing(self, x: pd.Series):\n",
        "        \"\"\"\n",
        "        This is a method that generates a sequential count of the periods for which a variable has been 1.\n",
        "\n",
        "        :param x: A pandas Series containing the target variable.\n",
        "\n",
        "        Returns:\n",
        "        - y (list): A list containing the sequential count of the periods for which the target variable has been 1.\n",
        "        \"\"\"\n",
        "\n",
        "        x = list(x)\n",
        "        y = []\n",
        "        episode_counter = 0\n",
        "        for n in range(0, len(x)):\n",
        "            if (x[n] == 0) & (n == 0):\n",
        "                y.append(episode_counter) # if it starts with no flows\n",
        "            elif x[n] == 1:\n",
        "                episode_counter += 1\n",
        "                y.append(episode_counter) # if there are flows\n",
        "            else:\n",
        "                y.append(0) # reset to 0 if no flows\n",
        "                episode_counter = 0\n",
        "        return y\n",
        "\n",
        "    def ongoing(self, input_df:pd.DataFrame, y_col:str, thresholds:list, shift_knowledge:int = None):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that represents a sequential count of the number of periods for which a variable has been above a given threshold.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create the count since variable.\n",
        "        :param thresholds: A list of thresholds to count since.\n",
        "        :param shift_knowledge: An integer defining by how many periods to shift the count since variable.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the count since variables appended.\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        binary_col_names = [y_col + '_above' + str(th) for th in thresholds]\n",
        "        col_names = [y_col + '_ongoing_' + str(th) for th in thresholds]\n",
        "\n",
        "        for idx, th in enumerate(thresholds):\n",
        "            df[binary_col_names[idx]] = (df[y_col] > th).astype(int)\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[binary_col_names[idx]].transform(self._count_ongoing)\n",
        "\n",
        "            if shift_knowledge is None:\n",
        "                pass\n",
        "            else:\n",
        "                #in case we need to shift by one since we don't know the y_col in current period\n",
        "                df[binary_col_names[idx]] = df.groupby(self.groupby_cols)[[binary_col_names[idx]]].shift(shift_knowledge)\n",
        "                df[col_names[idx]] = df.groupby(self.groupby_cols)[col_names[idx]].shift(shift_knowledge)\n",
        "        return df[[y_col, *[x for x in df.columns if 'ongoing' in x]]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTYb30yWdPQB"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-BOw2WN0jtO"
      },
      "outputs": [],
      "source": [
        "#add path to data and where you will save outputs\n",
        "data_path = \"\"\n",
        "output_path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqY4-JsfdQd-"
      },
      "outputs": [],
      "source": [
        "#read the data\n",
        "ucdp = pd.read_csv(f\"{data_path}/ucdp.csv\", index_col=0)\n",
        "ucdp = ucdp.set_index(['isocode', 'period']).sort_index() #set index to our id_vars and sort values\n",
        "\n",
        "#note: sorting values is CRITICAL when using group by operations\n",
        "ucdp\n",
        "\n",
        "#no need for population for demonstration. Keep it if you want a per capita target definition.\n",
        "ucdp = ucdp.drop(columns=[\"population\"])\n",
        "ucdp['fatalities_UCDP'] = ucdp['fatalities_UCDP'].astype(int)\n",
        "#rename column for convenience\n",
        "ucdp = ucdp.rename(columns={\"fatalities_UCDP\": \"violence\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpJnrVaFdEk7"
      },
      "source": [
        "# Construct panel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzYRl4N2dGuL"
      },
      "source": [
        "## Construct target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Hzn1h_odD-H"
      },
      "outputs": [],
      "source": [
        "#initialize our class\n",
        "te = TargetEngineer(df=ucdp, unit='isocode', time='period', y_col=\"violence\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahHNu0-7eBS8"
      },
      "outputs": [],
      "source": [
        "#set threshold and horizon to construct the target\n",
        "threshold = 0\n",
        "horizon = 3\n",
        "\n",
        "#get incidence and onset target dataframes\n",
        "inc_df = te.incidence(threshold=threshold, horizon=horizon)\n",
        "inc_target_col = 'inc_anyviolence_th0_h3'\n",
        "ons_df = te.onset(threshold=threshold, horizon=horizon)\n",
        "ons_target_col = 'ons_anyviolence_th0_h3'\n",
        "\n",
        "#save target and onset dfs\n",
        "inc_df.to_csv(f'{output_path}/incidence.csv')\n",
        "ons_df.to_csv(f'{output_path}/onset.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhlzWOiJep6a"
      },
      "source": [
        "## Construct features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgzfCQ3oerh4"
      },
      "outputs": [],
      "source": [
        "#instantitate feature engineer\n",
        "fe = FeatureEngineer(groupby_cols='isocode')\n",
        "y_col = 'violence' #same for all feature engineering\n",
        "windows = [1, 3, 12, 36, 60] #for rolling mean\n",
        "thresholds = [0] #for since and ongoing\n",
        "\n",
        "features = [\n",
        "    fe.rolling_mean(ucdp, y_col, windows).drop(columns = ['violence']),\n",
        "    fe.since(ucdp, y_col, thresholds).drop(columns = ['violence']),\n",
        "    fe.ongoing(ucdp, y_col, thresholds).drop(columns = ['violence']),\n",
        "    pd.read_csv(f'{data_path}/topics.csv', index_col=0).set_index(['isocode', 'period'])\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsPNbUgPgQn5"
      },
      "outputs": [],
      "source": [
        "X = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='inner'), features)\n",
        "\n",
        "#save features\n",
        "X.to_csv(f'{output_path}/features.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpS4TsVvguCl"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DOBEWQHgwvO"
      },
      "outputs": [],
      "source": [
        "#set key panelsplit parameters\n",
        "n_splits = 24\n",
        "test_size = 1\n",
        "gap = horizon - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcSHuBjjgvvt"
      },
      "source": [
        "## Incidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNlD_0degugQ"
      },
      "outputs": [],
      "source": [
        "#instantiate panelsplit\n",
        "inc_ps = PanelSplit(\n",
        "    periods = inc_df.index.get_level_values('period'),\n",
        "    n_splits=n_splits,\n",
        "    test_size=test_size,\n",
        "    gap = gap\n",
        ")\n",
        "\n",
        "inc_final_preds = inc_ps.gen_test_labels(inc_df.merge(X['violence_since_0'], left_index=True, right_index=True, how='left'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8vTVMN4i6iQ"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "\n",
        "inc_preds, fitted_estimators_inc = cross_val_fit_predict(\n",
        "    estimator=RandomForestClassifier(max_depth=4, max_features=0.2, min_samples_leaf=100, n_jobs=-1, random_state=42),\n",
        "    X=X,\n",
        "    y=inc_df[inc_target_col],\n",
        "    cv=inc_ps,\n",
        "    method='predict_proba',\n",
        "    drop_na_in_y=True\n",
        ")\n",
        "\n",
        "inc_final_preds['preds'] = inc_preds[:, 1]\n",
        "\n",
        "inc_final_preds.to_csv(f'{output_path}/incidence_predictions.csv')\n",
        "with open(f'{output_path}/fitted_estimators_inc.pkl', 'wb') as f:\n",
        "    pickle.dump(fitted_estimators_inc, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2roMkKnDkwK4"
      },
      "source": [
        "## Onset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwnW_PPmi6_I"
      },
      "outputs": [],
      "source": [
        "ons_ps = PanelSplit(\n",
        "    periods = ons_df.index.get_level_values('period'),\n",
        "    n_splits=n_splits,\n",
        "    test_size=test_size,\n",
        "    gap = gap\n",
        ")\n",
        "\n",
        "ons_final_preds = ons_ps.gen_test_labels(inc_df.merge(X['violence_since_0'], left_index=True, right_index=True, how='left'))\n",
        "\n",
        "ons_preds, fitted_estimators_inc = cross_val_fit_predict(\n",
        "    estimator=RandomForestClassifier(max_depth=4, max_features=0.2, min_samples_leaf=100, n_jobs=-1, random_state=42),\n",
        "    X=X,\n",
        "    y=ons_df\n",
        "    cv=ons_ps,\n",
        "    method='predict_proba',\n",
        "    drop_na_in_y=True\n",
        ")\n",
        "\n",
        "ons_final_preds['preds'] = ons_preds[:, 1]\n",
        "\n",
        "ons_final_preds.to_csv(f'{output_path}/onset_predictions.csv')\n",
        "with open(f'{output_path}/fitted_estimators_ons.pkl', 'wb') as f:\n",
        "    pickle.dump(fitted_estimators_inc, f)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
