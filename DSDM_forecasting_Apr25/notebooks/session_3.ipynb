{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUq6ibEJGPYF"
      },
      "outputs": [],
      "source": [
        "!pip install panelsplit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAbLtc0NGY0p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from panelsplit.cross_validation import PanelSplit\n",
        "from typing import Union\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZk2IZovprQn"
      },
      "outputs": [],
      "source": [
        "class FeatureEngineer:\n",
        "\n",
        "    \"\"\"\n",
        "    This is a class that contains general methods that can be applied to a DataFrame to create new features. Examples of such methods include creating lagged variables, rolling min/mean/max/sum and weighted rolling mean/sum.\n",
        "    The methods in this class are designed to be used in a pipeline to create new features for a given DataFrame.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    groupby_cols : Union[str, list]\n",
        "        A str or list of columns to group by\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "\n",
        "    lag(input_df:pd.DataFrame, y_col:str, lags:list):\n",
        "        This is a method that creates lagged variables for a given column in a DataFrame.\n",
        "\n",
        "    rolling_sum(input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "        This is a method that creates the rolling sum of specified windows for a given column in a DataFrame.\n",
        "\n",
        "    rolling_mean(input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "        This is a method that creates the rolling mean of specified windows for a given column in a DataFrame.\n",
        "\n",
        "    rolling_min(input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "        This is a method that creates the rolling min of specified windows for a given column in a DataFrame.\n",
        "\n",
        "    rolling_max(input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "        This is a method that creates the rolling max of specified windows for a given column in a DataFrame.\n",
        "\n",
        "    rolling_std(input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "        This is a method that creates the rolling standard deviation of specified windows for a given column in a DataFrame.\n",
        "\n",
        "    create_exponential_weights(window_size, alpha=0.8):\n",
        "        This is a method that enables generating \"rolling\" exponential weights for a given window size.\n",
        "\n",
        "    weighted_rolling_sum(input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False, alpha = 0.8):\n",
        "        This is a method that creates the weighted rolling sum of specified windows for a given column in a DataFrame.\n",
        "\n",
        "    weighted_rolling_mean(input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False, alpha = 0.8):\n",
        "        This is a method that creates the weighted rolling mean of specified windows for a given column in a DataFrame.\n",
        "\n",
        "    count_since(input_df:pd.DataFrame, y_col:str, thresholds:list, shift_knowledge:int = None):\n",
        "        This is a method that counts the number of periods since a variable has been above a given threshold.\n",
        "\n",
        "    ongoing(input_df:pd.DataFrame, y_col:str, thresholds:list, shift_knowledge:int = None):\n",
        "        This is a method that represents a sequential count of the number of periods for which a variable has been above a given threshold.\n",
        "\n",
        "    Notes:\n",
        "    -------\n",
        "    Be very careful with NAs when using the count_since_thresh and ongoing_episode methods.\n",
        "    The way we are computing things here (i.e. using a > th condition) means they are treated as a 0/False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, groupby_cols: Union[str, list]):\n",
        "\n",
        "        self.groupby_cols = groupby_cols\n",
        "\n",
        "    def _index_check(self, df:pd.DataFrame):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that checks if the index of a DataFrame is sorted correctly.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param df: The DataFrame to check.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The DataFrame with a sorted index.\n",
        "        \"\"\"\n",
        "\n",
        "        assert df.index.is_monotonic_increasing, \"The index of the DataFrame should be monotonically increasing.\"\n",
        "\n",
        "    def lag(self, input_df:pd.DataFrame, y_col:str, lags:list):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that creates lagged variables for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create lagged variables.\n",
        "        :param lags: A list of lag values to create.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the lagged variables appended.\n",
        "        \"\"\"\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_basic_lag' + str(lag) for lag in lags]\n",
        "        for idx, lag in enumerate(lags):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].shift(lag)\n",
        "        return df\n",
        "\n",
        "    def rolling_sum(self, input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that creates the rolling sum of specified windows for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create lagged variables.\n",
        "        :param groupby_cols: A list of columns to group by.\n",
        "        :param windows: A list of windows to generate a rolling sum for.\n",
        "        :param closed: A string indicating the side of the window interval to close on. Closed = 'left' omits the current observation.\n",
        "        :param return_logs: A boolean indicating whether to return the log of the rolling sum.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the rolling sum variables appended.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_rolling_sum' + str(w) for w in windows]\n",
        "\n",
        "        for idx, w in enumerate(windows):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].rolling(w, min_periods=1, closed = closed).sum().values\n",
        "            if return_logs:\n",
        "                df['ln_' + col_names[idx]] = np.log1p(df[col_names[idx]])\n",
        "                df = df.drop(col_names[idx], axis = 1)\n",
        "        return df\n",
        "\n",
        "    def rolling_mean(self, input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that creates the rolling mean of specified windows for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create rolling variables.\n",
        "        :param windows: A list of windows to generate a rolling mean for.\n",
        "        :param closed: A string indicating the side of the window interval to close on. Closed = 'left' omits the current observation.\n",
        "        :param return_logs: A boolean indicating whether to return the log of the rolling mean.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the rolling mean variables appended.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_rolling_mean' + str(w) for w in windows]\n",
        "\n",
        "        for idx, w in enumerate(windows):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].rolling(w, min_periods=1, closed = closed).mean().values\n",
        "            if return_logs:\n",
        "                df['ln_' + col_names[idx]] = np.log1p(df[col_names[idx]])\n",
        "                df = df.drop(col_names[idx], axis = 1)\n",
        "        return df\n",
        "\n",
        "    def rolling_min(self, input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that creates the rolling min of specified windows for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create rolling variables.\n",
        "        :param windows: A list of windows to generate a rolling min for.\n",
        "        :param closed: A string indicating the side of the window interval to close on. Closed = 'left' omits the current observation.\n",
        "        :param return_logs: A boolean indicating whether to return the log of the rolling min.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the rolling min variables appended.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_rolling_min' + str(w) for w in windows]\n",
        "\n",
        "        for idx, w in enumerate(windows):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].rolling(w, min_periods=1, closed = closed).min().values\n",
        "            if return_logs:\n",
        "                df['ln_' + col_names[idx]] = np.log1p(df[col_names[idx]])\n",
        "                df = df.drop(col_names[idx], axis = 1)\n",
        "        return df\n",
        "\n",
        "    def rolling_max(self, input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that creates the rolling max of specified windows for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create rolling variables.\n",
        "        :param windows: A list of windows to generate a rolling max for.\n",
        "        :param closed: A string indicating the side of the window interval to close on. Closed = 'left' omits the current observation.\n",
        "        :param return_logs: A boolean indicating whether to return the log of the rolling max.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the rolling max variables appended.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_rolling_max' + str(w) for w in windows]\n",
        "\n",
        "        for idx, w in enumerate(windows):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].rolling(w, min_periods=1, closed = closed).max().values\n",
        "            if return_logs:\n",
        "                df['ln_' + col_names[idx]] = np.log1p(df[col_names[idx]])\n",
        "                df = df.drop(col_names[idx], axis = 1)\n",
        "        return df\n",
        "\n",
        "    def rolling_std(self, input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that creates the rolling standard deviation of specified windows for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create rolling variables.\n",
        "        :param windows: A list of windows to generate a rolling standard deviation for.\n",
        "        :param closed: A string indicating the side of the window interval to close on. Closed = 'left' omits the current observation.\n",
        "        :param return_logs: A boolean indicating whether to return the log of the rolling standard deviation.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the rolling standard deviation variables appended.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_rolling_std' + str(w) for w in windows]\n",
        "\n",
        "        for idx, w in enumerate(windows):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].rolling(w, min_periods=1, closed = closed).std().values\n",
        "            if return_logs:\n",
        "                df['ln_' + col_names[idx]] = np.log1p(df[col_names[idx]])\n",
        "                df = df.drop(col_names[idx], axis = 1)\n",
        "        return df\n",
        "\n",
        "    def _create_exponential_weights(self, window_size, alpha=0.8):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that enables generating \"rolling\" exponential weights for a given window size.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param window_size: The size of the window for which weights are calculated.\n",
        "        :param alpha: The decay factor for weights, defaults to 0.5.\n",
        "                    A higher alpha discounts older observations faster.\n",
        "\n",
        "        Returns:\n",
        "        -----\n",
        "        :return: A numpy array of weights.\n",
        "        \"\"\"\n",
        "\n",
        "        weights = alpha ** np.arange(window_size)\n",
        "        normalized_weights = weights / weights.sum()\n",
        "        return normalized_weights[::-1]\n",
        "\n",
        "    def weighted_rolling_sum(self, input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False, alpha = 0.8):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that creates the weighted rolling sum of specified windows for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create weighted rolling variables.\n",
        "        :param groupby_cols: A list of columns to group by.\n",
        "        :param windows: A list of windows to generate a weighted rolling sum for.\n",
        "        :param closed: A string indicating the side of the window interval to close on. Closed = 'left' omits the current observation.\n",
        "        :param return_logs:  A boolean indicating whether to return the log of the weighted rolling sum.\n",
        "        :param alpha: The decay factor for weights, defaults to 0.8. A higher alpha discounts older observations faster.\n",
        "\n",
        "        Returns:\n",
        "        -----\n",
        "        :return: The original DataFrame with the weighted rolling sum variables appended.\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_weighted_rolling_sum' + str(w) for w in windows]\n",
        "\n",
        "        for idx, w in enumerate(windows):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].rolling(w, min_periods=1, closed = closed).apply(lambda x: np.sum(self._create_exponential_weights(len(x), alpha) * x), raw = True).values\n",
        "            if return_logs:\n",
        "                df['ln_' + col_names[idx]] = np.log1p(df[col_names[idx]])\n",
        "                df = df.drop(col_names[idx], axis = 1)\n",
        "        return df\n",
        "\n",
        "    def weighted_rolling_mean(self, input_df:pd.DataFrame, y_col:str, windows:list, closed = None, return_logs = False, alpha = 0.8):\n",
        "        \"\"\"\n",
        "        This is a method that creates the weighted rolling mean of specified windows for a given column in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create weighted rolling variables.\n",
        "        :param windows: A list of windows to generate a weighted rolling mean for.\n",
        "        :param closed: A string indicating the side of the window interval to close on. Closed = 'left' omits the current observation.\n",
        "        :param return_logs:  A boolean indicating whether to return the log of the weighted rolling mean.\n",
        "        :param alpha: The decay factor for weights, defaults to 0.8. A higher alpha discounts older observations faster.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the weighted rolling mean variables appended.\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        col_names = [y_col + '_weighted_rolling_mean' + str(w) for w in windows]\n",
        "\n",
        "        for idx, w in enumerate(windows):\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[y_col].rolling(w, min_periods=1, closed = closed).apply(lambda x: np.sum(self._create_exponential_weights(len(x), alpha) * x) / len(x), raw = True).values\n",
        "            if return_logs:\n",
        "                df['ln_' + col_names[idx]] = np.log1p(df[col_names[idx]])\n",
        "                df = df.drop(col_names[idx], axis = 1)\n",
        "        return df\n",
        "\n",
        "    def _count_since(self, x: pd.Series):\n",
        "        \"\"\"\n",
        "        This is a method that counts the number of periods since a variable has been 1.\n",
        "\n",
        "        :param x: A pandas Series containing the target variable.\n",
        "\n",
        "        Returns:\n",
        "        - y (list): A list containing the number of periods since the target variable has been 1.\n",
        "        \"\"\"\n",
        "\n",
        "        x = list(x)\n",
        "        y = []\n",
        "        for n in range(0, len(x)):\n",
        "            if (x[n] == 0) & (n == 0):\n",
        "                y.append(1) # if it starts with no flows\n",
        "            elif x[n] == 1:\n",
        "                y.append(0) # reset to 0 if flows\n",
        "            else:\n",
        "                y.append(y[n-1]+1) # add 1 if no flows\n",
        "        return y\n",
        "\n",
        "    def since(self, input_df:pd.DataFrame, y_col:str, thresholds:list, shift_knowledge:int = None):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that counts the number of periods since a variable has been above a given threshold.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create the count since variable.\n",
        "        :param thresholds: A list of thresholds to count since.\n",
        "        :param shift_knowledge: An integer defining by how many periods to shift the count since variable.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the count since variables appended.\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "\n",
        "        binary_col_names = [y_col + '_above' + str(th) for th in thresholds]\n",
        "        col_names = [y_col + '_since_' + str(th) for th in thresholds]\n",
        "\n",
        "        for idx, th in enumerate(thresholds):\n",
        "            df[binary_col_names[idx]] = (df[y_col] > th).astype(int)\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[binary_col_names[idx]].transform(self._count_since)\n",
        "\n",
        "            if shift_knowledge is None:\n",
        "                pass\n",
        "            else:\n",
        "                #in case we need to shift by one since we don't know the y_col in current period\n",
        "                df[binary_col_names[idx]] = df.groupby(self.groupby_cols)[[binary_col_names[idx]]].shift(shift_knowledge)\n",
        "                df[col_names[idx]] = df.groupby(self.groupby_cols)[col_names[idx]].shift(shift_knowledge)\n",
        "        return df[[y_col, *[x for x in df.columns if 'since' in x]]]\n",
        "\n",
        "    def _count_ongoing(self, x: pd.Series):\n",
        "        \"\"\"\n",
        "        This is a method that generates a sequential count of the periods for which a variable has been 1.\n",
        "\n",
        "        :param x: A pandas Series containing the target variable.\n",
        "\n",
        "        Returns:\n",
        "        - y (list): A list containing the sequential count of the periods for which the target variable has been 1.\n",
        "        \"\"\"\n",
        "\n",
        "        x = list(x)\n",
        "        y = []\n",
        "        episode_counter = 0\n",
        "        for n in range(0, len(x)):\n",
        "            if (x[n] == 0) & (n == 0):\n",
        "                y.append(episode_counter) # if it starts with no flows\n",
        "            elif x[n] == 1:\n",
        "                episode_counter += 1\n",
        "                y.append(episode_counter) # if there are flows\n",
        "            else:\n",
        "                y.append(0) # reset to 0 if no flows\n",
        "                episode_counter = 0\n",
        "        return y\n",
        "\n",
        "    def ongoing(self, input_df:pd.DataFrame, y_col:str, thresholds:list, shift_knowledge:int = None):\n",
        "\n",
        "        \"\"\"\n",
        "        This is a method that represents a sequential count of the number of periods for which a variable has been above a given threshold.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        :param input_df: The DataFrame containing the data.\n",
        "        :param y_col: The name of the column for which to create the count since variable.\n",
        "        :param thresholds: A list of thresholds to count since.\n",
        "        :param shift_knowledge: An integer defining by how many periods to shift the count since variable.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        :return: The original DataFrame with the count since variables appended.\n",
        "        \"\"\"\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        self._index_check(df)\n",
        "\n",
        "        binary_col_names = [y_col + '_above' + str(th) for th in thresholds]\n",
        "        col_names = [y_col + '_ongoing_' + str(th) for th in thresholds]\n",
        "\n",
        "        for idx, th in enumerate(thresholds):\n",
        "            df[binary_col_names[idx]] = (df[y_col] > th).astype(int)\n",
        "            df[col_names[idx]] = df.groupby(self.groupby_cols)[binary_col_names[idx]].transform(self._count_ongoing)\n",
        "\n",
        "            if shift_knowledge is None:\n",
        "                pass\n",
        "            else:\n",
        "                #in case we need to shift by one since we don't know the y_col in current period\n",
        "                df[binary_col_names[idx]] = df.groupby(self.groupby_cols)[[binary_col_names[idx]]].shift(shift_knowledge)\n",
        "                df[col_names[idx]] = df.groupby(self.groupby_cols)[col_names[idx]].shift(shift_knowledge)\n",
        "        return df[[y_col, *[x for x in df.columns if 'ongoing' in x]]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91u9hLZ7qcQU"
      },
      "source": [
        "Bonus:\n",
        "- Instead of directly using my class, it would be good for your learning to try implementing at least some of these operations for yourself. It's the best way to learn!\n",
        "- Advanced Python users can check out the feature-engine (https://feature-engine.trainindata.com/en/latest/) package. It's a more sophisticated way to develop custom classes for feature engineering.\n",
        "- Advanced Python users can also check out dataclasses (https://www.datacamp.com/tutorial/python-data-classes). This is a more modern way of building classes in Python. I have been meaning to transition to this, but never found the time..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_kgxt32GWdn"
      },
      "source": [
        "# (1) Feature engineering for UCDP data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4PHvqwE2h9k"
      },
      "source": [
        "The objective of this section is to introduce classic time-series feature engineering techniques for panel data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydsAjaJFpnoE"
      },
      "source": [
        "## Load data and initialize feature engineer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwboVm9wpmIN"
      },
      "outputs": [],
      "source": [
        "#NB - upload the ucdp.csv file into files (toggle on the left-hand side). You will need to do this for any given run of the notebook.\n",
        "#alternatively upload to your G Drive and mount your drive in this notebook\n",
        "import os\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hmAHUDfpus6"
      },
      "outputs": [],
      "source": [
        "#read the data\n",
        "ucdp = pd.read_csv(\"ucdp.csv\", index_col = 0)\n",
        "\n",
        "#note: sorting values is CRITICAL when using group by operations\n",
        "ucdp = ucdp.set_index(['isocode', 'period']).sort_index() #set index to our id_vars and sort values\n",
        "\n",
        "#drop population for now - we don't need it\n",
        "ucdp = ucdp.drop(columns = ['population'])\n",
        "\n",
        "#rename column\n",
        "ucdp = ucdp.rename(columns = {'fatalities_UCDP': 'violence'})\n",
        "\n",
        "#display\n",
        "ucdp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXxJRMiYqw_F"
      },
      "outputs": [],
      "source": [
        "fe = FeatureEngineer(groupby_cols = 'isocode')\n",
        "\n",
        "#the class is flexible - but for this notebook we will only ever by working with this column for operations\n",
        "y_col = 'violence'\n",
        "\n",
        "#build an example condition which we will use throughout\n",
        "example_iso = 'BFA'\n",
        "example_periods = np.arange(201601, 201813)\n",
        "example_cond = (ucdp.index.get_level_values('isocode') == example_iso) & (ucdp.index.get_level_values('period').isin(example_periods))\n",
        "\n",
        "#display example\n",
        "ucdp[example_cond]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbJxOJ2NpQVI"
      },
      "source": [
        "## Continuous features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPV4xL6crZEm"
      },
      "source": [
        "### Basic lag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93QQOTkyraoF"
      },
      "outputs": [],
      "source": [
        "#very simple 1, 3 and 5 month lags\n",
        "lags = [1, 3, 6]\n",
        "\n",
        "lag_df = fe.lag(ucdp, y_col, lags)\n",
        "\n",
        "lag_df[example_cond]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOSardLxtQeT"
      },
      "source": [
        "Things you should notice:\n",
        "\n",
        "- Checking NAs in the dataframe. When do they appear? Why? Hint:\n",
        "\n",
        "`lag_df[lag_df['violence_basic_lag1'].isna()].reset_index()['period'].unique())`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UYMBoSAqmsR"
      },
      "source": [
        "### Rolling sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q7Bj5phICfe"
      },
      "outputs": [],
      "source": [
        "windows = [1, 3, 6]\n",
        "closed = None\n",
        "return_logs = False\n",
        "\n",
        "rolling_sum_df = fe.rolling_sum(ucdp, y_col, windows, closed, return_logs)\n",
        "\n",
        "rolling_sum_df[example_cond]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiaUPyuJwTin"
      },
      "source": [
        "Things you should notice:\n",
        "- I am always setting min_periods = 1 in the class. This means we only require 1 observation to compute any rolling value. If it's not obvious, check the pandas documentation to understand better https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html.\n",
        "\n",
        "Try:\n",
        "  - Changing `closed = \"left\"`. How does this affect the rolling computation. In what situations would you want this property?\n",
        "  - Other rolling operations e.g. mean, min, max.\n",
        "\n",
        "Bonus:\n",
        "- In what situations would you transform the feature to log terms? Why? Would it make theoretically any difference for tree-based methods?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5YFobkAzgFb"
      },
      "source": [
        "### Weighted rolling sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R95JmWMVze91"
      },
      "outputs": [],
      "source": [
        "#here I am showing you the contribution to the sum for each timestep using a window = 3 and alpha = 0.8\n",
        "alpha = 0.8\n",
        "\n",
        "fe._create_exponential_weights(window_size = 3, alpha = alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA_aCcdF0Kk-"
      },
      "source": [
        "Try:\n",
        "  - Changing `alpha` and `window_size`. How do the weights change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPPIa9JW0Q1d"
      },
      "outputs": [],
      "source": [
        "windows = [1, 3, 6]\n",
        "closed = None\n",
        "return_logs = False\n",
        "alpha = 0.2\n",
        "\n",
        "weighted_rolling_sum_df = fe.weighted_rolling_sum(ucdp, y_col, windows, closed, return_logs, alpha)\n",
        "\n",
        "weighted_rolling_sum_df[example_cond]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpldewMS14dK"
      },
      "source": [
        "Try:\n",
        "  - Changing `alpha`. How does this affect the rolling computation. Why would you set alpha higher or lower?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81l51PgIzcJF"
      },
      "source": [
        "## Discrete features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW6JaEDC2Zfe"
      },
      "source": [
        "## Since"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqwanFJox-11"
      },
      "outputs": [],
      "source": [
        "thresholds = [0, 10, 100]\n",
        "shift_knowledge = None\n",
        "since_df = fe.since(ucdp, y_col, thresholds, shift_knowledge)\n",
        "\n",
        "since_df[example_cond]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aot9ZlhJY5e2"
      },
      "source": [
        "## Ongoing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etNTb-7HY7aj"
      },
      "outputs": [],
      "source": [
        "thresholds = [0, 10, 100]\n",
        "shift_knowledge = None\n",
        "ongoing_df = fe.ongoing(ucdp, y_col, thresholds, shift_knowledge)\n",
        "\n",
        "ongoing_df[example_cond]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgqHCuamZMqp"
      },
      "source": [
        "Try:\n",
        "- Setting `shift_knowledge = 1` for since/ongoing. How does the feature value change? Does it remind you of a parameter we have for rolling features? When would you use it?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t-zqDip2s6W"
      },
      "source": [
        "# (2) Feature engineering for LDA topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABv2lnO_2vzE"
      },
      "outputs": [],
      "source": [
        "raw_topics_lby = pd.read_csv('raw_topics_lby.csv', index_col = 0).sort_values(by = ['isocode', 'period']) #raw topics, only includes LBY - full history (198901 to 202412)\n",
        "topics = pd.read_csv('topics.csv', index_col = 0).sort_values(by = ['isocode', 'period']) #stock topics, all countries (201001 to 202412)\n",
        "\n",
        "#subset to example isocode and 201001 to 202412\n",
        "example_iso = 'LBY'\n",
        "topics_lby = topics[(topics['isocode'] == example_iso)].sort_values(by = ['isocode', 'period'])\n",
        "raw_topics_lby = raw_topics_lby[raw_topics_lby['period'].isin(topics_lby['period'].unique())].sort_values(by = ['isocode', 'period'])\n",
        "\n",
        "#display\n",
        "display(topics_lby)\n",
        "display(raw_topics_lby)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbH0oLwca_jt"
      },
      "outputs": [],
      "source": [
        "#plot tokens vs stock tokens\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6), dpi=300)\n",
        "\n",
        "topic_idx = 2\n",
        "\n",
        "timesteps = pd.Index(pd.date_range(start=\"2010-01-01\", periods=180, freq=\"MS\").strftime(\"%Y%m\"))\n",
        "raw_topics_lby['plot_period'] = timesteps\n",
        "topics_lby['plot_period'] = timesteps\n",
        "\n",
        "sns.lineplot(data=raw_topics_lby, x='plot_period', y=f'pr_topic_{topic_idx}', label='Raw topic', ax=ax, alpha = 0.6)\n",
        "sns.lineplot(data=topics_lby, x='plot_period', y=f'stock_topic_{topic_idx}', label='Stock topic', ax=ax)\n",
        "\n",
        "#set xticks every 12\n",
        "ax.set_xticks(np.arange(0, 180, 12), labels=timesteps[np.arange(0, 180, 12)], rotation=90)\n",
        "\n",
        "ax.set_xlabel(\"\")\n",
        "ax.set_ylabel(\"Topic share\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fao-y8DicPKU"
      },
      "source": [
        "Bonus:\n",
        "- Can you code up your own functions to compute the stock topic share? Use the formulas provided in class as reference. Start with the raw_topics.csv (Libya only), apply your function/s and they should be the same as the stock_topic columns (for Libya) in topics.csv.\n",
        "- If you can do this for one topic column, one country time series then it would be trivial for you to manage this for all columns, all countries!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9R0qVMc22Cz"
      },
      "source": [
        "# (3) Generate predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZWKeYGOqKO7"
      },
      "source": [
        "Below is a quick intro to the panelsplit cross_val_fit_predict method\n",
        "\n",
        "\n",
        "```\n",
        "from panelsplit.application import cross_val_fit_predict\n",
        "\n",
        "preds, fitted_estimators = cross_val_fit_predict(\n",
        "    estimator=, #your ML model of choice e.g. RandomForestClassifier()\n",
        "    X=, #your feature dataframe\n",
        "    y= ,#your target e.g. target_df[target_col]\n",
        "    cv=, #your cross-validation strategy. All you need is your initialized panelsplit object e.g. PanelSplit(periods, n_splits, test_size, gap)\n",
        "    method=, #'predict' or 'predict_proba'\n",
        "    drop_na_in_y= #whether to drop NAs if they are still present in your target\n",
        ")\n",
        "\n",
        "Returns\n",
        "-------\n",
        "\n",
        "preds: np.array\n",
        "  numpy array of your predictions\n",
        "fitted_estimators: list\n",
        "  a list of fitted models for each split\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1dDSHeaqe_n"
      },
      "source": [
        "# Task for next session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2sOVB-C29S3"
      },
      "source": [
        "Using the session_2 and session_3 notebooks, your objective is to generate pseudo-out-of-sample predictions. The specification of the task is as follows:\n",
        "\n",
        "- **Target variable:** Anyviolence for a 3 month forecasting horizon. This is a a classification task. Please generate predictions for both incidence and onset.\n",
        "- **Test period:** Your test predictions should start in 202301. You should generate pseudo-out-of-sample predictions for every month (`test_size=1`) up to and including 202412. Remember to set gap correctly.\n",
        "- **Features**: As a minimum, your feature set should be as follows:\n",
        "\n",
        "a) Rolling mean of fatalities for window sizes of [1, 3, 12, 36, 60]. Switch this to weighted if you prefer.\n",
        "\n",
        "b) Number of months since violence exceeed 0 fatalities\n",
        "\n",
        "c) Periods of consecutive months exceeding 0 fatalities (ongoing)\n",
        "\n",
        "d) LDA stock topics (you can use topics.csv, these have the stock computation already applied)\n",
        "\n",
        "As a default please use:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(max_depth=4, max_features=0.2, min_samples_leaf=100, random_state=42)\n",
        "```\n",
        "\n",
        "You are free to experiment with additional features/models if you like! Do not invest time in hyperparameter tuning.\n",
        "\n",
        "**Hints:**\n",
        "\n",
        "- Check out PanelSplit.gen_test_labels() --> this is a really easy way for you to collect predictions across every split. Make sure you retain the `since` column in your final predictions dataframe - you will need this for evaluation.\n",
        "\n",
        "```\n",
        "ps = PanelSplit(periods, n_splits, test_size, gap)\n",
        "final_preds_df = ps.gen_test_labels(target_df.merge(X['violence_since_0'], left_index=True, right_index=True, how='left')) #this will generate a dataframe where you can save down your predictions. Retains the since column also.\n",
        "final_preds_df['preds'] = preds[:, 1] #here I am saving the predicted probabilities for class 1 (assumes method is 'predict_proba')\n",
        "```\n",
        "\n",
        "- The default method in `cross_val_fit_predict` is `predict`. Since this is a classification task, you will want `predict_proba`.\n",
        "- Check out the `drop_na_in_y` parameter in the `cross_val_fit_predict` method of `panelsplit.application`. This will avoid:\n",
        "\n",
        "```\n",
        "ValueError: Input y contains NaN.\n",
        "```\n",
        "\n",
        "Make sure you understand what is going on under the hood. Check the source code!\n",
        "\n",
        "**Bonus:**\n",
        "\n",
        "- Run models using only historical violence features and only text features. In the next session you will learn how to evaluate performance. Then you can compare the models you already ran (i.e. all features) with the history-only and text-only features?\n",
        "- Same as above, but now run for for a 12 month, rather than a 3 month horizon.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
